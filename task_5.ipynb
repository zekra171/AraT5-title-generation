{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install accelerate\n",
        "!pip install nltk\n",
        "!pip install datasets\n",
        "!pip install regex\n",
        "!pip install sklearn\n",
        "!pip install sentencepiece\n",
        "!pip install protobuf\n",
        "!pip install fairscale\n",
        "!pip install sacrebleu\n",
        "!pip install rouge_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyRzbVObo-oL",
        "outputId": "9c40ab49-3838-4631-917b-bb196cdf770a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.32.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.5)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Collecting requests>=2.32.2 (from datasets)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.5.0,>=2023.1.0 (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, requests, pyarrow, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.6.1\n",
            "    Uninstalling fsspec-2024.6.1:\n",
            "      Successfully uninstalled fsspec-2024.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "gcsfs 2024.6.1 requires fsspec==2024.6.1, but you have fsspec 2024.5.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.20.0 dill-0.3.8 fsspec-2024.5.0 multiprocess-0.70.16 pyarrow-17.0.0 requests-2.32.3 xxhash-3.4.1\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2024.5.15)\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31mÃ—\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31mâ”‚\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31mâ•°â”€>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31mÃ—\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31mâ•°â”€>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (3.20.3)\n",
            "Collecting fairscale\n",
            "  Downloading fairscale-0.4.13.tar.gz (266 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m266.3/266.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from fairscale) (2.3.1+cu121)\n",
            "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from fairscale) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->fairscale) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->fairscale) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->fairscale) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->fairscale) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->fairscale) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->fairscale) (2024.5.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->fairscale) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->fairscale) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->fairscale) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->fairscale) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->fairscale) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->fairscale) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->fairscale) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->fairscale) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->fairscale) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->fairscale) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->fairscale) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->fairscale) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->fairscale) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->fairscale) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->fairscale) (1.3.0)\n",
            "Building wheels for collected packages: fairscale\n",
            "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.13-py3-none-any.whl size=332106 sha256=ee9fce4aae3a2544bc6420321512a5b96210a02e4581479e7b78787d11815e2b\n",
            "  Stored in directory: /root/.cache/pip/wheels/78/a4/c0/fb0a7ef03cff161611c3fa40c6cf898f76e58ec421b88e8cb3\n",
            "Successfully built fairscale\n",
            "Installing collected packages: fairscale\n",
            "Successfully installed fairscale-0.4.13\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.4.2-py3-none-any.whl.metadata (58 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.0/58.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2024.5.15)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.26.4)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.4)\n",
            "Downloading sacrebleu-2.4.2-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.6 portalocker-2.10.1 sacrebleu-2.4.2\n",
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.4)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=b9ad13305914e9db5df56085673c8869d44c0a7be92d51cc2ef0bb2c5eff79d9\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/UBC-NLP/araT5/main/examples/run_trainier_seq2seq_huggingface.py?token=AA4R7KKKCBPS5WX4BJSWNFTBG2OPK -O run_trainier_seq2seq_huggingface.py\n",
        "!wget https://raw.githubusercontent.com/UBC-NLP/araT5/main/examples/eval_squad.py?token=AA4R7KOBBXSPCDXRRHJ3RW3BG2RF4 -O eval_squad.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpA_CZdIm4KJ",
        "outputId": "ca6af0bf-5be8-4265-d4ca-53d4e2fd2035"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-30 07:42:22--  https://raw.githubusercontent.com/UBC-NLP/araT5/main/examples/run_trainier_seq2seq_huggingface.py?token=AA4R7KKKCBPS5WX4BJSWNFTBG2OPK\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 36637 (36K) [text/plain]\n",
            "Saving to: â€˜run_trainier_seq2seq_huggingface.pyâ€™\n",
            "\n",
            "\r          run_train   0%[                    ]       0  --.-KB/s               \rrun_trainier_seq2se 100%[===================>]  35.78K  --.-KB/s    in 0.008s  \n",
            "\n",
            "2024-07-30 07:42:23 (4.32 MB/s) - â€˜run_trainier_seq2seq_huggingface.pyâ€™ saved [36637/36637]\n",
            "\n",
            "--2024-07-30 07:42:23--  https://raw.githubusercontent.com/UBC-NLP/araT5/main/examples/eval_squad.py?token=AA4R7KOBBXSPCDXRRHJ3RW3BG2RF4\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7689 (7.5K) [text/plain]\n",
            "Saving to: â€˜eval_squad.pyâ€™\n",
            "\n",
            "eval_squad.py       100%[===================>]   7.51K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-07-30 07:42:23 (86.3 MB/s) - â€˜eval_squad.pyâ€™ saved [7689/7689]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_trainier_seq2seq_huggingface.py \\\n",
        "        --learning_rate 5e-5 \\\n",
        "        --max_target_length 128 --max_source_length 128 \\\n",
        "        --per_device_train_batch_size 8 --per_device_eval_batch_size 8 \\\n",
        "        --model_name_or_path \"UBC-NLP/AraT5-base\" \\\n",
        "        --output_dir \"/content/AraT5_FT_title_generation\" --overwrite_output_dir \\\n",
        "        --num_train_epochs 22 \\\n",
        "        --train_file \"/content/ARGEn_title_genration_sample_train.tsv\" \\\n",
        "        --validation_file \"/content/ARGEn_title_genration_sample_valid.tsv\" \\\n",
        "        --task \"title_generation\" --text_column \"document\" --summary_column \"title\" \\\n",
        "        --load_best_model_at_end --metric_for_best_model \"eval_bleu\" --greater_is_better True --evaluation_strategy steps --logging_strategy steps --predict_with_generate \\\n",
        "        --do_train --do_eval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJbEv7wSnEvR",
        "outputId": "204e66ff-87e3-4382-8d33-417f427fc919"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-30 07:45:27.199540: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-30 07:45:27.199596: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-30 07:45:27.200890: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-30 07:45:27.207848: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-30 07:45:28.312740: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "last_checkpoint None\n",
            "07/30/2024 07:45:30 - WARNING - __main__ -   Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n",
            "07/30/2024 07:45:30 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=500,\n",
            "eval_strategy=steps,\n",
            "evaluation_strategy=steps,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/AraT5_FT_title_generation/runs/Jul30_07-45-30_6b222b895e3a,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=eval_bleu,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=22.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=/content/AraT5_FT_title_generation,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "predict_with_generate=True,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/AraT5_FT_title_generation,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "[INFO] loading from TSV\n",
            "loading configuration file config.json from cache at /tmp/AraT5_cache_dir/models--UBC-NLP--AraT5-base/snapshots/b99444fa197df5a598e0a426e76a31e4a43d1d23/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"UBC-NLP/AraT5-base\",\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 110080\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /tmp/AraT5_cache_dir/models--UBC-NLP--AraT5-base/snapshots/b99444fa197df5a598e0a426e76a31e4a43d1d23/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"UBC-NLP/AraT5-base\",\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 110080\n",
            "}\n",
            "\n",
            "loading file spiece.model from cache at /tmp/AraT5_cache_dir/models--UBC-NLP--AraT5-base/snapshots/b99444fa197df5a598e0a426e76a31e4a43d1d23/spiece.model\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /tmp/AraT5_cache_dir/models--UBC-NLP--AraT5-base/snapshots/b99444fa197df5a598e0a426e76a31e4a43d1d23/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /tmp/AraT5_cache_dir/models--UBC-NLP--AraT5-base/snapshots/b99444fa197df5a598e0a426e76a31e4a43d1d23/tokenizer_config.json\n",
            "loading configuration file config.json from cache at /tmp/AraT5_cache_dir/models--UBC-NLP--AraT5-base/snapshots/b99444fa197df5a598e0a426e76a31e4a43d1d23/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"UBC-NLP/AraT5-base\",\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 110080\n",
            "}\n",
            "\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "loading configuration file config.json from cache at /tmp/AraT5_cache_dir/models--UBC-NLP--AraT5-base/snapshots/b99444fa197df5a598e0a426e76a31e4a43d1d23/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"UBC-NLP/AraT5-base\",\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 110080\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /tmp/AraT5_cache_dir/models--UBC-NLP--AraT5-base/snapshots/b99444fa197df5a598e0a426e76a31e4a43d1d23/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at UBC-NLP/AraT5-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
            "Generation config file not found, using a generation config created from the model config.\n",
            "Map:   0% 0/100 [00:00<?, ? examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4016: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "Map: 100% 100/100 [00:00<00:00, 835.45 examples/s]\n",
            "[INFO] evlaute using  bleu score task name: title_generation\n",
            "[INFO] early_stopping_num= 20\n",
            "***** checkpoint= None\n",
            "***** Running training *****\n",
            "  Num examples = 1,000\n",
            "  Num Epochs = 22\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 2,750\n",
            "  Number of trainable parameters = 282,770,688\n",
            "{'loss': 12.8931, 'grad_norm': 3.134603977203369, 'learning_rate': 4.0909090909090915e-05, 'epoch': 4.0}\n",
            " 18% 500/2750 [03:18<15:57,  2.35it/s]\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 100\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "\n",
            "  0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            " 15% 2/13 [00:00<00:03,  3.43it/s]\u001b[A\n",
            " 23% 3/13 [00:01<00:04,  2.47it/s]\u001b[A\n",
            " 31% 4/13 [00:01<00:04,  2.10it/s]\u001b[A\n",
            " 38% 5/13 [00:02<00:04,  1.96it/s]\u001b[A\n",
            " 46% 6/13 [00:02<00:03,  1.87it/s]\u001b[A\n",
            " 54% 7/13 [00:03<00:03,  1.83it/s]\u001b[A\n",
            " 62% 8/13 [00:04<00:02,  1.80it/s]\u001b[A\n",
            " 69% 9/13 [00:04<00:02,  1.78it/s]\u001b[A\n",
            " 77% 10/13 [00:05<00:01,  1.77it/s]\u001b[A\n",
            " 85% 11/13 [00:05<00:01,  1.74it/s]\u001b[A\n",
            " 92% 12/13 [00:06<00:00,  1.75it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 7.639238357543945, 'eval_bleu': 0.0647, 'eval_gen_len': 18.38, 'eval_runtime': 7.6509, 'eval_samples_per_second': 13.07, 'eval_steps_per_second': 1.699, 'epoch': 4.0}\n",
            " 18% 500/2750 [03:26<15:57,  2.35it/s]\n",
            "100% 13/13 [00:06<00:00,  1.85it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to /content/AraT5_FT_title_generation/checkpoint-500\n",
            "Configuration saved in /content/AraT5_FT_title_generation/checkpoint-500/config.json\n",
            "Configuration saved in /content/AraT5_FT_title_generation/checkpoint-500/generation_config.json\n",
            "Model weights saved in /content/AraT5_FT_title_generation/checkpoint-500/model.safetensors\n",
            "tokenizer config file saved in /content/AraT5_FT_title_generation/checkpoint-500/tokenizer_config.json\n",
            "Special tokens file saved in /content/AraT5_FT_title_generation/checkpoint-500/special_tokens_map.json\n",
            "Copy vocab file to /content/AraT5_FT_title_generation/checkpoint-500/spiece.model\n",
            "{'loss': 7.6315, 'grad_norm': 3.8657100200653076, 'learning_rate': 3.181818181818182e-05, 'epoch': 8.0}\n",
            " 36% 1000/2750 [07:10<11:57,  2.44it/s]\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 100\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "\n",
            "  0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            " 15% 2/13 [00:00<00:04,  2.64it/s]\u001b[A\n",
            " 23% 3/13 [00:01<00:05,  1.91it/s]\u001b[A\n",
            " 31% 4/13 [00:02<00:04,  1.83it/s]\u001b[A\n",
            " 38% 5/13 [00:02<00:04,  1.79it/s]\u001b[A\n",
            " 46% 6/13 [00:03<00:03,  1.77it/s]\u001b[A\n",
            " 54% 7/13 [00:03<00:03,  1.74it/s]\u001b[A\n",
            " 62% 8/13 [00:04<00:02,  1.74it/s]\u001b[A\n",
            " 69% 9/13 [00:04<00:02,  1.73it/s]\u001b[A\n",
            " 77% 10/13 [00:05<00:01,  1.72it/s]\u001b[A\n",
            " 85% 11/13 [00:06<00:01,  1.74it/s]\u001b[A\n",
            " 92% 12/13 [00:06<00:00,  1.73it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 6.94691801071167, 'eval_bleu': 1.4069, 'eval_gen_len': 15.27, 'eval_runtime': 7.9635, 'eval_samples_per_second': 12.557, 'eval_steps_per_second': 1.632, 'epoch': 8.0}\n",
            " 36% 1000/2750 [07:18<11:57,  2.44it/s]\n",
            "100% 13/13 [00:07<00:00,  1.83it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to /content/AraT5_FT_title_generation/checkpoint-1000\n",
            "Configuration saved in /content/AraT5_FT_title_generation/checkpoint-1000/config.json\n",
            "Configuration saved in /content/AraT5_FT_title_generation/checkpoint-1000/generation_config.json\n",
            "Model weights saved in /content/AraT5_FT_title_generation/checkpoint-1000/model.safetensors\n",
            "tokenizer config file saved in /content/AraT5_FT_title_generation/checkpoint-1000/tokenizer_config.json\n",
            "Special tokens file saved in /content/AraT5_FT_title_generation/checkpoint-1000/special_tokens_map.json\n",
            "Copy vocab file to /content/AraT5_FT_title_generation/checkpoint-1000/spiece.model\n",
            "{'loss': 6.9705, 'grad_norm': 5.121058940887451, 'learning_rate': 2.272727272727273e-05, 'epoch': 12.0}\n",
            " 55% 1500/2750 [11:17<08:24,  2.48it/s]\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 100\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "\n",
            "  0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            " 15% 2/13 [00:00<00:03,  3.46it/s]\u001b[A\n",
            " 23% 3/13 [00:01<00:04,  2.40it/s]\u001b[A\n",
            " 31% 4/13 [00:01<00:04,  2.09it/s]\u001b[A\n",
            " 38% 5/13 [00:02<00:04,  1.94it/s]\u001b[A\n",
            " 46% 6/13 [00:02<00:03,  1.86it/s]\u001b[A\n",
            " 54% 7/13 [00:03<00:03,  1.81it/s]\u001b[A\n",
            " 62% 8/13 [00:04<00:02,  1.77it/s]\u001b[A\n",
            " 69% 9/13 [00:04<00:02,  1.76it/s]\u001b[A\n",
            " 77% 10/13 [00:05<00:01,  1.73it/s]\u001b[A\n",
            " 85% 11/13 [00:05<00:01,  1.72it/s]\u001b[A\n",
            " 92% 12/13 [00:06<00:00,  1.73it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 6.596485614776611, 'eval_bleu': 1.8732, 'eval_gen_len': 16.27, 'eval_runtime': 7.5744, 'eval_samples_per_second': 13.202, 'eval_steps_per_second': 1.716, 'epoch': 12.0}\n",
            " 55% 1500/2750 [11:25<08:24,  2.48it/s]\n",
            "100% 13/13 [00:06<00:00,  1.83it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to /content/AraT5_FT_title_generation/checkpoint-1500\n",
            "Configuration saved in /content/AraT5_FT_title_generation/checkpoint-1500/config.json\n",
            "Configuration saved in /content/AraT5_FT_title_generation/checkpoint-1500/generation_config.json\n",
            "Model weights saved in /content/AraT5_FT_title_generation/checkpoint-1500/model.safetensors\n",
            "tokenizer config file saved in /content/AraT5_FT_title_generation/checkpoint-1500/tokenizer_config.json\n",
            "Special tokens file saved in /content/AraT5_FT_title_generation/checkpoint-1500/special_tokens_map.json\n",
            "Copy vocab file to /content/AraT5_FT_title_generation/checkpoint-1500/spiece.model\n",
            "{'loss': 6.5427, 'grad_norm': 6.14951229095459, 'learning_rate': 1.3636363636363637e-05, 'epoch': 16.0}\n",
            " 73% 2000/2750 [15:03<05:07,  2.44it/s]\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 100\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "\n",
            "  0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            " 15% 2/13 [00:00<00:03,  3.36it/s]\u001b[A\n",
            " 23% 3/13 [00:01<00:04,  2.39it/s]\u001b[A\n",
            " 31% 4/13 [00:01<00:04,  2.09it/s]\u001b[A\n",
            " 38% 5/13 [00:02<00:04,  1.96it/s]\u001b[A\n",
            " 46% 6/13 [00:02<00:03,  1.87it/s]\u001b[A\n",
            " 54% 7/13 [00:03<00:03,  1.83it/s]\u001b[A\n",
            " 62% 8/13 [00:04<00:02,  1.80it/s]\u001b[A\n",
            " 69% 9/13 [00:04<00:02,  1.78it/s]\u001b[A\n",
            " 77% 10/13 [00:05<00:01,  1.75it/s]\u001b[A\n",
            " 85% 11/13 [00:05<00:01,  1.76it/s]\u001b[A\n",
            " 92% 12/13 [00:06<00:00,  1.74it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 6.402466297149658, 'eval_bleu': 1.8106, 'eval_gen_len': 16.09, 'eval_runtime': 7.6124, 'eval_samples_per_second': 13.136, 'eval_steps_per_second': 1.708, 'epoch': 16.0}\n",
            " 73% 2000/2750 [15:11<05:07,  2.44it/s]\n",
            "100% 13/13 [00:07<00:00,  1.71it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to /content/AraT5_FT_title_generation/checkpoint-2000\n",
            "Configuration saved in /content/AraT5_FT_title_generation/checkpoint-2000/config.json\n",
            "Configuration saved in /content/AraT5_FT_title_generation/checkpoint-2000/generation_config.json\n",
            "Model weights saved in /content/AraT5_FT_title_generation/checkpoint-2000/model.safetensors\n",
            "tokenizer config file saved in /content/AraT5_FT_title_generation/checkpoint-2000/tokenizer_config.json\n",
            "Special tokens file saved in /content/AraT5_FT_title_generation/checkpoint-2000/special_tokens_map.json\n",
            "Copy vocab file to /content/AraT5_FT_title_generation/checkpoint-2000/spiece.model\n",
            "{'loss': 6.2988, 'grad_norm': 5.509311199188232, 'learning_rate': 4.5454545454545455e-06, 'epoch': 20.0}\n",
            " 91% 2500/2750 [19:06<01:40,  2.49it/s]\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 100\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "\n",
            "  0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            " 15% 2/13 [00:00<00:03,  3.38it/s]\u001b[A\n",
            " 23% 3/13 [00:01<00:04,  2.42it/s]\u001b[A\n",
            " 31% 4/13 [00:01<00:04,  2.07it/s]\u001b[A\n",
            " 38% 5/13 [00:02<00:04,  1.94it/s]\u001b[A\n",
            " 46% 6/13 [00:02<00:03,  1.86it/s]\u001b[A\n",
            " 54% 7/13 [00:03<00:03,  1.80it/s]\u001b[A\n",
            " 62% 8/13 [00:04<00:02,  1.79it/s]\u001b[A\n",
            " 69% 9/13 [00:04<00:02,  1.76it/s]\u001b[A\n",
            " 77% 10/13 [00:05<00:01,  1.75it/s]\u001b[A\n",
            " 85% 11/13 [00:05<00:01,  1.75it/s]\u001b[A\n",
            " 92% 12/13 [00:06<00:00,  1.74it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 6.341480731964111, 'eval_bleu': 1.8031, 'eval_gen_len': 15.4, 'eval_runtime': 7.588, 'eval_samples_per_second': 13.179, 'eval_steps_per_second': 1.713, 'epoch': 20.0}\n",
            " 91% 2500/2750 [19:13<01:40,  2.49it/s]\n",
            "100% 13/13 [00:06<00:00,  1.85it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to /content/AraT5_FT_title_generation/checkpoint-2500\n",
            "Configuration saved in /content/AraT5_FT_title_generation/checkpoint-2500/config.json\n",
            "Configuration saved in /content/AraT5_FT_title_generation/checkpoint-2500/generation_config.json\n",
            "Model weights saved in /content/AraT5_FT_title_generation/checkpoint-2500/model.safetensors\n",
            "tokenizer config file saved in /content/AraT5_FT_title_generation/checkpoint-2500/tokenizer_config.json\n",
            "Special tokens file saved in /content/AraT5_FT_title_generation/checkpoint-2500/special_tokens_map.json\n",
            "Copy vocab file to /content/AraT5_FT_title_generation/checkpoint-2500/spiece.model\n",
            "100% 2750/2750 [21:20<00:00,  2.46it/s]Saving model checkpoint to /content/AraT5_FT_title_generation/checkpoint-2750\n",
            "Configuration saved in /content/AraT5_FT_title_generation/checkpoint-2750/config.json\n",
            "Configuration saved in /content/AraT5_FT_title_generation/checkpoint-2750/generation_config.json\n",
            "Model weights saved in /content/AraT5_FT_title_generation/checkpoint-2750/model.safetensors\n",
            "tokenizer config file saved in /content/AraT5_FT_title_generation/checkpoint-2750/tokenizer_config.json\n",
            "Special tokens file saved in /content/AraT5_FT_title_generation/checkpoint-2750/special_tokens_map.json\n",
            "Copy vocab file to /content/AraT5_FT_title_generation/checkpoint-2750/spiece.model\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from /content/AraT5_FT_title_generation/checkpoint-1500 (score: 1.8732).\n",
            "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
            "{'train_runtime': 1307.0571, 'train_samples_per_second': 16.832, 'train_steps_per_second': 2.104, 'train_loss': 7.8965211736505685, 'epoch': 22.0}\n",
            "100% 2750/2750 [21:47<00:00,  2.10it/s]\n",
            "Saving model checkpoint to /content/AraT5_FT_title_generation\n",
            "Configuration saved in /content/AraT5_FT_title_generation/config.json\n",
            "Configuration saved in /content/AraT5_FT_title_generation/generation_config.json\n",
            "Model weights saved in /content/AraT5_FT_title_generation/model.safetensors\n",
            "tokenizer config file saved in /content/AraT5_FT_title_generation/tokenizer_config.json\n",
            "Special tokens file saved in /content/AraT5_FT_title_generation/special_tokens_map.json\n",
            "Copy vocab file to /content/AraT5_FT_title_generation/spiece.model\n",
            "07/30/2024 08:07:37 - INFO - __main__ -   ***** train metrics *****\n",
            "07/30/2024 08:07:37 - INFO - __main__ -     epoch                    =       22.0\n",
            "07/30/2024 08:07:37 - INFO - __main__ -     total_flos               =  3119249GF\n",
            "07/30/2024 08:07:37 - INFO - __main__ -     train_loss               =     7.8965\n",
            "07/30/2024 08:07:37 - INFO - __main__ -     train_runtime            = 0:21:47.05\n",
            "07/30/2024 08:07:37 - INFO - __main__ -     train_samples            =       1000\n",
            "07/30/2024 08:07:37 - INFO - __main__ -     train_samples_per_second =     16.832\n",
            "07/30/2024 08:07:37 - INFO - __main__ -     train_steps_per_second   =      2.104\n",
            "07/30/2024 08:07:37 - INFO - __main__ -   *** Evaluate ***\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 100\n",
            "  Batch size = 8\n",
            "100% 13/13 [00:37<00:00,  2.88s/it]\n",
            "07/30/2024 08:08:18 - INFO - __main__ -   ***** val metrics *****\n",
            "07/30/2024 08:08:18 - INFO - __main__ -     epoch                   =       22.0\n",
            "07/30/2024 08:08:18 - INFO - __main__ -     eval_bleu               =     0.8575\n",
            "07/30/2024 08:08:18 - INFO - __main__ -     eval_check_point        = AraT5-base\n",
            "07/30/2024 08:08:18 - INFO - __main__ -     eval_gen_len            =      84.51\n",
            "07/30/2024 08:08:18 - INFO - __main__ -     eval_loss               =     6.5965\n",
            "07/30/2024 08:08:18 - INFO - __main__ -     eval_runtime            = 0:00:41.32\n",
            "07/30/2024 08:08:18 - INFO - __main__ -     eval_samples            =        100\n",
            "07/30/2024 08:08:18 - INFO - __main__ -     eval_samples_per_second =       2.42\n",
            "07/30/2024 08:08:18 - INFO - __main__ -     eval_steps_per_second   =      0.315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_trainier_seq2seq_huggingface.py \\\n",
        "        --learning_rate 5e-5 \\\n",
        "        --max_target_length 128 --max_source_length 128 \\\n",
        "        --per_device_train_batch_size 8 --per_device_eval_batch_size 8 \\\n",
        "        --model_name_or_path \"/content/AraT5_FT_title_generation\" \\\n",
        "        --output_dir \"/content/AraT5_FT_title_generation\" \\\n",
        "        --train_file \"/content/ARGEn_title_genration_sample_train.tsv\" \\\n",
        "        --validation_file \"/content/ARGEn_title_genration_sample_valid.tsv\" \\\n",
        "        --test_file \"/content/ARGEn_title_genration_sample_valid.tsv\" \\\n",
        "        --metric_for_best_model \"eval_bleu\" \\\n",
        "        --task \"title_generation\" --text_column \"document\" --summary_column \"title\" \\\n",
        "        --predict_with_generate --do_predict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nS5pu0hBnMso",
        "outputId": "c66e3ee1-260b-4f93-cb18-f7ec1bf42319"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-30 08:10:27.935062: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-30 08:10:27.935106: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-30 08:10:27.936493: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-30 08:10:27.944139: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-30 08:10:29.094150: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "last_checkpoint /content/AraT5_FT_title_generation/checkpoint-2750\n",
            "07/30/2024 08:10:30 - WARNING - __main__ -   Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n",
            "07/30/2024 08:10:30 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=False,\n",
            "do_predict=True,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=None,\n",
            "eval_strategy=no,\n",
            "evaluation_strategy=None,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/AraT5_FT_title_generation/runs/Jul30_08-10-30_6b222b895e3a,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=eval_bleu,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=/content/AraT5_FT_title_generation,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "predict_with_generate=True,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/AraT5_FT_title_generation,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "[INFO] loading from TSV\n",
            "Generating train split: 1000 examples [00:00, 9589.26 examples/s]\n",
            "Generating validation split: 100 examples [00:00, 9325.44 examples/s]\n",
            "Generating test split: 100 examples [00:00, 9805.50 examples/s]\n",
            "loading configuration file /content/AraT5_FT_title_generation/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"/content/AraT5_FT_title_generation\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 110080\n",
            "}\n",
            "\n",
            "loading file spiece.model\n",
            "loading file tokenizer.json\n",
            "loading file added_tokens.json\n",
            "loading file special_tokens_map.json\n",
            "loading file tokenizer_config.json\n",
            "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
            "loading weights file /content/AraT5_FT_title_generation/model.safetensors\n",
            "Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at /content/AraT5_FT_title_generation.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
            "loading configuration file /content/AraT5_FT_title_generation/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "Map:   0% 0/100 [00:00<?, ? examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4016: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "Map: 100% 100/100 [00:00<00:00, 888.13 examples/s]\n",
            "[INFO] evlaute using  bleu score task name: title_generation\n",
            "[INFO] early_stopping_num= 20\n",
            "07/30/2024 08:10:34 - INFO - __main__ -   *** Test ***\n",
            "\n",
            "***** Running Prediction *****\n",
            "  Num examples = 100\n",
            "  Batch size = 8\n",
            "100% 13/13 [00:36<00:00,  2.84s/it]\n",
            "07/30/2024 08:11:16 - INFO - __main__ -   ***** test metrics *****\n",
            "07/30/2024 08:11:16 - INFO - __main__ -     test_bleu               =                    0.8575\n",
            "07/30/2024 08:11:16 - INFO - __main__ -     test_check_point        = AraT5_FT_title_generation\n",
            "07/30/2024 08:11:16 - INFO - __main__ -     test_gen_len            =                     84.51\n",
            "07/30/2024 08:11:16 - INFO - __main__ -     test_loss               =                    6.5965\n",
            "07/30/2024 08:11:16 - INFO - __main__ -     test_runtime            =                0:00:42.35\n",
            "07/30/2024 08:11:16 - INFO - __main__ -     test_samples            =                       100\n",
            "07/30/2024 08:11:16 - INFO - __main__ -     test_samples_per_second =                     2.361\n",
            "07/30/2024 08:11:16 - INFO - __main__ -     test_steps_per_second   =                     0.307\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_trainier_seq2seq_huggingface.py \\\n",
        "        --learning_rate 5e-5 \\\n",
        "        --max_target_length 128 --max_source_length 128 \\\n",
        "        --per_device_train_batch_size 8 --per_device_eval_batch_size 8 \\\n",
        "        --model_name_or_path \"UBC-NLP/AraT5-base-title-generation\" \\\n",
        "        --output_dir \"/content/AraT5-base-title-generation_output\" \\\n",
        "        --train_file \"/content/ARGEn_title_genration_sample_train.tsv\" \\\n",
        "        --validation_file \"/content/ARGEn_title_genration_sample_valid.tsv\" \\\n",
        "        --test_file \"/content/ARGEn_title_genration_sample_valid.tsv\" \\\n",
        "        --metric_for_best_model \"eval_bleu\" \\\n",
        "        --task \"title_generation\" --text_column \"document\" --summary_column \"title\" \\\n",
        "        --predict_with_generate --do_predict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XxTKA0YnQ7U",
        "outputId": "30339c79-fdac-4fa0-fcd7-09a7e78d0f53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-30 08:11:42.662310: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-30 08:11:42.662365: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-30 08:11:42.664276: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-30 08:11:42.675656: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-30 08:11:43.788117: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "last_checkpoint None\n",
            "07/30/2024 08:11:45 - WARNING - __main__ -   Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n",
            "07/30/2024 08:11:45 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=False,\n",
            "do_predict=True,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=None,\n",
            "eval_strategy=no,\n",
            "evaluation_strategy=None,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/AraT5-base-title-generation_output/runs/Jul30_08-11-45_6b222b895e3a,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=eval_bleu,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=/content/AraT5-base-title-generation_output,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "predict_with_generate=True,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/AraT5-base-title-generation_output,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "[INFO] loading from TSV\n",
            "config.json: 100% 687/687 [00:00<00:00, 4.93MB/s]\n",
            "loading configuration file config.json from cache at /tmp/AraT5_cache_dir/models--UBC-NLP--AraT5-base-title-generation/snapshots/f26ed5960b5ccff858860ab346040dd1a05d032e/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"UBC-NLP/AraT5-base-title-generation\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 110080\n",
            "}\n",
            "\n",
            "tokenizer_config.json: 100% 81.0/81.0 [00:00<00:00, 565kB/s]\n",
            "loading configuration file config.json from cache at /tmp/AraT5_cache_dir/models--UBC-NLP--AraT5-base-title-generation/snapshots/f26ed5960b5ccff858860ab346040dd1a05d032e/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"UBC-NLP/AraT5-base-title-generation\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 110080\n",
            "}\n",
            "\n",
            "spiece.model: 100% 2.44M/2.44M [00:00<00:00, 47.6MB/s]\n",
            "special_tokens_map.json: 100% 98.0/98.0 [00:00<00:00, 819kB/s]\n",
            "loading file spiece.model from cache at /tmp/AraT5_cache_dir/models--UBC-NLP--AraT5-base-title-generation/snapshots/f26ed5960b5ccff858860ab346040dd1a05d032e/spiece.model\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /tmp/AraT5_cache_dir/models--UBC-NLP--AraT5-base-title-generation/snapshots/f26ed5960b5ccff858860ab346040dd1a05d032e/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /tmp/AraT5_cache_dir/models--UBC-NLP--AraT5-base-title-generation/snapshots/f26ed5960b5ccff858860ab346040dd1a05d032e/tokenizer_config.json\n",
            "loading configuration file config.json from cache at /tmp/AraT5_cache_dir/models--UBC-NLP--AraT5-base-title-generation/snapshots/f26ed5960b5ccff858860ab346040dd1a05d032e/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"UBC-NLP/AraT5-base-title-generation\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 110080\n",
            "}\n",
            "\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "loading configuration file config.json from cache at /tmp/AraT5_cache_dir/models--UBC-NLP--AraT5-base-title-generation/snapshots/f26ed5960b5ccff858860ab346040dd1a05d032e/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"UBC-NLP/AraT5-base-title-generation\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 110080\n",
            "}\n",
            "\n",
            "pytorch_model.bin: 100% 1.13G/1.13G [00:09<00:00, 116MB/s] \n",
            "loading weights file pytorch_model.bin from cache at /tmp/AraT5_cache_dir/models--UBC-NLP--AraT5-base-title-generation/snapshots/f26ed5960b5ccff858860ab346040dd1a05d032e/pytorch_model.bin\n",
            "Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at UBC-NLP/AraT5-base-title-generation.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
            "Generation config file not found, using a generation config created from the model config.\n",
            "Map:   0% 0/100 [00:00<?, ? examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4016: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "Map: 100% 100/100 [00:00<00:00, 820.41 examples/s]\n",
            "[INFO] evlaute using  bleu score task name: title_generation\n",
            "[INFO] early_stopping_num= 20\n",
            "07/30/2024 08:12:00 - INFO - __main__ -   *** Test ***\n",
            "\n",
            "***** Running Prediction *****\n",
            "  Num examples = 100\n",
            "  Batch size = 8\n",
            "100% 13/13 [00:07<00:00,  1.75it/s]Traceback (most recent call last):\n",
            "  File \"/content/run_trainier_seq2seq_huggingface.py\", line 807, in <module>\n",
            "    main()\n",
            "  File \"/content/run_trainier_seq2seq_huggingface.py\", line 764, in main\n",
            "    test_results = trainer.predict(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer_seq2seq.py\", line 244, in predict\n",
            "    return super().predict(test_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 3717, in predict\n",
            "    output = eval_loop(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 3923, in evaluation_loop\n",
            "    metrics = self.compute_metrics(EvalPrediction(predictions=all_preds, label_ids=all_labels))\n",
            "  File \"/content/run_trainier_seq2seq_huggingface.py\", line 590, in compute_metrics\n",
            "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\", line 3866, in batch_decode\n",
            "    return [\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\", line 3867, in <listcomp>\n",
            "    self.decode(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\", line 3906, in decode\n",
            "    return self._decode(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_fast.py\", line 651, in _decode\n",
            "    text = self._tokenizer.decode(token_ids, skip_special_tokens=skip_special_tokens)\n",
            "OverflowError: out of range integral type conversion attempted\n",
            "100% 13/13 [00:08<00:00,  1.60it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#top 10 titles genrated from our full finetuned model (UBC-NLP/AraT5-base-title-generation)\n",
        "!head -10 /content/AraT5_FT_title_generation/test_preds_seq2seq.txt"
      ],
      "metadata": {
        "id": "LsGrrBit8exc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf49ff8e-cbc8-4de9-91c8-9c758e17990a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ù†Ù‚Ù„: 'Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Â»: ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ù†Ù‚Ù„: ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ù†Ù‚Ù„: 'Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Â»: 'Ø§Ù„Ø§Ù„Ø§Ù„Â»: 'Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Â»: 'Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Â»: 'Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Â»: 'Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Â»: 'Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Â»: 'Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Â»: 'Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Â»: 'Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Â»: 'Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„': 'Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Â»: 'Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„': 'Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„ Ø§Ù„Ø³ÙŠØ§Ø±Ø§Øª: ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø¥Ù‚ØªØµØ§Ø¯ÙŠØ©Â»: ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø¥Ù‚ØªØµØ§Ø¯ÙŠØ©Â»: 'Ø§Ù„Ø§Ù„\n",
            "Ø§Ù„Ø¨Ù†Ùƒ Ø§Ù„Ø³ÙˆØ¯Ø§Ù†: Ø£Ø³Ø¹Ø§Ø± Ø§Ù„Ø¨Ù†ÙˆÙƒ Ø§Ù„Ø³ÙˆØ¯Ø§Ù†: Ø£Ø³Ø¹Ø§Ø± Ø§Ù„Ø¹Ù…Ù„Ø§Øª Ø§Ù„Ø£Ø¬Ù†Ø¨ÙŠØ© ÙÙŠ Ø§Ù„Ø³ÙˆÙ‚ Ø§Ù„Ø³ÙˆØ¯Ø§Ù†\n",
            "Â«Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„\n",
            "Ø£Ø³Ø¹Ø§Ø± Ø§Ù„Ø¹Ù…Ù„Ø§Øª Ø§Ù„Ø§Ø¬Ù†Ø¨ÙŠØ© Ù…Ù‚Ø§Ø¨Ù„ Ø§Ù„Ø¬Ù†ÙŠÙ‡ Ø§Ù„Ø³ÙˆØ¯Ø§Ù†ÙŠ Ù„ÙŠÙˆÙ… Ø§Ù„Ø«Ù„Ø§Ø«Ø§Ø¡ Ø§Ù„Ù…ÙˆØ§ÙÙ‚ Ø§Ù„Ù…ÙˆØ§ÙÙ‚Ø£ÙƒØªÙˆØ¨Ø±2017\n",
            "Â«Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„ Ø§Ù„Ø¥Ø³ÙƒØ§Ù†Â»:Â»:Â»:Â»:Â»:Â»:Â»\n",
            "Ø§Ù„Ø°Ù‡Ø¨ ÙŠØ±ØªÙØ¹ ÙÙŠ Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹\n",
            "ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø¶Ø±Ø§Ø¦Ø¨ Ø§Ù„ØªØ±ÙƒÙŠØ©:  Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„\n",
            "ÙˆØ²Ø§Ø±Ø© ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ù†ÙØ·Â»: Â«Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„Ø§Ù„\n",
            "ÙˆØ²ÙŠØ± Ø§Ù„Ø·Ø§Ù‚Ø©: Ù…Ø¬Ù„Ø³ Ø§Ù„Ø³ÙˆØ¯Ø§Ù†: Ù…Ø¬Ù„Ø³ ÙˆØ²ÙŠØ± Ø§Ù„Ø¥Ù‚ØªØµØ§Ø¯ÙŠØ©: Ù…Ø¬Ù„Ø³ Ø§Ù„Ø³ÙˆØ¯Ø§Ù†: Ù…Ø¬Ù„Ø³ ÙˆØ²ÙŠØ± Ø§Ù„Ø¥Ù‚ØªØµØ§Ø¯ÙŠØ©: Ù…Ø¬Ù„Ø³ ÙˆØ²ÙŠØ± Ø§Ù„Ø¥Ù‚ØªØµØ§Ø¯ÙŠØ©: Ù…Ø¬Ù„Ø³ Ø§Ù„Ø³ÙˆØ¯Ø§Ù†: Ù…Ø¬Ù„Ø³ ÙˆØ²ÙŠØ± Ø§Ù„Ø¥Ù‚ØªØµØ§Ø¯ÙŠØ©: Ù…Ø¬Ù„Ø³ ÙˆØ²ÙŠØ± Ø§Ù„Ø¥Ù‚ØªØµØ§Ø¯ÙŠØ©: Ù…Ø¬Ù„Ø³ ÙˆØ²ÙŠØ± Ø§Ù„Ø¥Ù‚ØªØµØ§Ø¯ÙŠØ©: Ù…Ø¬Ù„Ø³ ÙˆØ²ÙŠØ± Ø§Ù„Ø¥Ù‚ØªØµØ§Ø¯ÙŠØ©: Ù…Ø¬Ù„Ø³ Ø§Ù„Ø³ÙˆØ¯Ø§Ù†: Ù…Ø¬Ù„Ø³ ÙˆØ²ÙŠØ± Ø§Ù„Ø¥Ù‚ØªØµØ§Ø¯ÙŠØ©: Ù…Ø¬Ù„Ø³ ÙˆØ²ÙŠØ± Ø§Ù„Ø¥Ù‚ØªØµØ§Ø¯ÙŠØ©: Ù…Ø¬Ù„Ø³ Ø§Ù„Ø³ÙˆØ¯Ø§Ù†: Ù…Ø¬Ù„Ø³ ÙˆØ²ÙŠØ± Ø§Ù„Ø¥Ù‚ØªØµØ§Ø¯ÙŠØ©: Ù…Ø¬Ù„Ø³ ÙˆØ²ÙŠØ± Ø§Ù„Ø¥Ù‚ØªØµØ§Ø¯ÙŠØ©: Ù…Ø¬Ù„Ø³ Ø§Ù„Ø³ÙˆØ¯Ø§Ù†: Ù…Ø¬Ù„Ø³ ÙˆØ²ÙŠØ± Ø§Ù„Ø¥Ù‚ØªØµØ§Ø¯ÙŠØ©: Ù…Ø¬Ù„Ø³ Ø§Ù„Ø³ÙˆØ¯Ø§Ù†: Ù…Ø¬Ù„Ø³ ÙˆØ²ÙŠØ± Ø§Ù„Ø¥Ù‚ØªØµØ§Ø¯ÙŠØ©: Ù…Ø¬Ù„Ø³ Ø§Ù„Ø³ÙˆØ¯Ø§Ù†: Ù…Ø¬Ù„Ø³ ÙˆØ²ÙŠØ± Ø§Ù„Ø¥Ù‚ØªØµØ§Ø¯ÙŠØ©: Ù…Ø¬Ù„Ø³ Ø§Ù„Ø³ÙˆØ¯Ø§Ù†: Ù…Ø¬Ù„Ø³ ÙˆØ²ÙŠØ± Ø§Ù„Ø¥Ù‚ØªØµØ§Ø¯ÙŠØ©: Ù…Ø¬Ù„Ø³ Ø§Ù„Ø³ÙˆØ¯Ø§Ù†: Ù…Ø¬Ù„Ø³ ÙˆØ²ÙŠØ± Ø§Ù„Ø¥Ù‚ØªØµØ§Ø¯ÙŠØ©: Ù…Ø¬Ù„Ø³ Ø§Ù„Ø³ÙˆØ¯Ø§Ù†: Ù…Ø¬Ù„Ø³ ÙˆØ²ÙŠØ± Ø§Ù„Ø¥Ù‚ØªØµØ§Ø¯ÙŠØ©: Ù…Ø¬Ù„Ø³ Ø§Ù„Ø³ÙˆØ¯Ø§Ù†: Ù…Ø¬Ù„Ø³ ÙˆØ²ÙŠØ± Ø§Ù„Ø¥Ù‚ØªØµØ§Ø¯ÙŠØ©: Ù…Ø¬Ù„Ø³ Ø§Ù„Ø³ÙˆØ¯Ø§Ù†:::::: Ù…Ø¬Ù„Ø³ Ø§Ù„Ø³ÙˆØ¯Ø§Ù†: Ù…Ø¬Ù„Ø³ Ø§Ù„Ø³ÙˆØ¯Ø§Ù†: Ù…Ø¬Ù„Ø³ Ø§Ù„Ø³ÙˆØ¯Ø§Ù†\n",
            "ÙˆØ²ÙŠØ± Ø§Ù„Ù…Ø§Ù„ÙŠØ©:: Ù…Ù† Ù‚Ø±ÙˆØ¶ Ù‚Ø±ÙˆØ¶ Ù‚Ø±ÙˆØ¶ Ù‚Ø±ÙˆØ¶ Ù‚Ø±ÙˆØ¶ Ù…Ø§Ù„ÙŠØ©  Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„Ù„: Ø§Ù„Ù…Ø§Ù„ÙŠØ©::::: ÙŠ:\n"
          ]
        }
      ]
    }
  ]
}